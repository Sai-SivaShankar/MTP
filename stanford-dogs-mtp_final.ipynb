{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":308849,"sourceType":"datasetVersion","datasetId":129000},{"sourceId":169320,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":144065,"modelId":166641}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# # Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# # Correct paths to the train and test directories\n# train_dir = \"/kaggle/input/stanford-dogs-dataset-traintest/cropped/train/\"\n# test_dir = \"/kaggle/input/stanford-dogs-dataset-traintest/cropped/test/\"\n\n# # Data transformations\n# transform = transforms.Compose([\n#     transforms.Resize((64, 64)),  # Resize images to 224x224\n#     transforms.ToTensor(),         # Convert images to tensors\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n# ])\n\n# # Load datasets\n# train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n# test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n\n# # Data loaders\n# train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n# test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n\n# Class names and mapping\n# print(f\"Classes: {train_data.classes}\")\n# print(f\"Class-to-index mapping: {train_data.class_to_idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T12:38:25.254544Z","iopub.execute_input":"2024-11-27T12:38:25.254922Z","iopub.status.idle":"2024-11-27T12:38:25.261368Z","shell.execute_reply.started":"2024-11-27T12:38:25.254890Z","shell.execute_reply":"2024-11-27T12:38:25.260453Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"# # Load pretrained ResNet50 model\n# model = models.resnet50(pretrained=True)\n\n# # Modify the last fully connected layer to match the number of classes in your dataset\n# num_ftrs = model.fc.in_features\n# model.fc = nn.Linear(num_ftrs, len(train_data.classes))  # Adjust for the number of classes\n\n# # Move model to GPU if available\n# model = model.to(device)\n\n# # Define loss function and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.0001)\"\"\n\n# # Training loop\n# num_epochs = 10\n# for epoch in range(num_epochs):\n#     model.train()  # Set model to training mode\n#     running_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     # Iterate over training data\n#     for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n#         inputs, labels = inputs.to(device), labels.to(device)\n        \n#         optimizer.zero_grad()  # Zero the gradients\n#         outputs = model(inputs)  # Forward pass\n#         loss = criterion(outputs, labels)  # Calculate loss\n#         loss.backward()  # Backpropagation\n#         optimizer.step()  # Update weights\n        \n#         running_loss += loss.item()\n#         _, predicted = torch.max(outputs, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum().item()\n    \n#     epoch_loss = running_loss / len(train_loader)\n#     epoch_acc = correct / total\n#     print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}\")\n    \n#     # Validation loop\n#     model.eval()  # Set model to evaluation mode\n#     val_loss = 0.0\n#     val_correct = 0\n#     val_total = 0\n    \n#     with torch.no_grad():  # No need to track gradients for validation\n#         for inputs, labels in test_loader:\n#             inputs, labels = inputs.to(device), labels.to(device)\n            \n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n            \n#             val_loss += loss.item()\n#             _, predicted = torch.max(outputs, 1)\n#             val_total += labels.size(0)\n#             val_correct += (predicted == labels).sum().item()\n    \n#     val_loss = val_loss / len(test_loader)\n#     val_acc = val_correct / val_total\n#     print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n\n# # Save the trained model\n# torch.save(model.state_dict(), 'resnet50_dog_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:38:25.262749Z","iopub.execute_input":"2024-11-27T12:38:25.263007Z","iopub.status.idle":"2024-11-27T12:38:25.284047Z","shell.execute_reply.started":"2024-11-27T12:38:25.262982Z","shell.execute_reply":"2024-11-27T12:38:25.283240Z"},"trusted":true},"outputs":[],"execution_count":105},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define Gaussian blur function\ndef gaussian_blur(image, ksize=3):\n    return cv2.GaussianBlur(image, (ksize, ksize), 2.7, 9)\n\n# Define the custom Dataset for blurred images\nclass BlurredDogDataset(Dataset):\n    def __init__(self, dataset, blur_ksize=3):\n        self.dataset = dataset\n        self.blur_ksize = blur_ksize\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        original_image, label = self.dataset[idx]\n        original_image_np = np.array(original_image)  # Convert to numpy array\n  \n        # Apply Gaussian blur\n        blurred_image_np = gaussian_blur(original_image_np, self.blur_ksize)\n       \n        # Add Gaussian noise\n        gaussian_noise = 0.1 * np.random.randn(*blurred_image_np.shape)\n        blurred_image_np += gaussian_noise\n        # gaussian_noise = 0.2 * np.random.randn(*blurred_image_np.shape)\n        # blurred_image_np += gaussian_noise\n        # Convert back to tensors (ensure they are in the (C, H, W) format)\n        blurred_image = torch.tensor(blurred_image_np).float()  # Convert to tensor\n        original_image = torch.tensor(original_image_np).float()  # Convert to tensor\n        \n        # Ensure the images are in the (C, H, W) format (channels first)\n        blurred_image = blurred_image.permute(0, 1,2)  # Convert to (C, H, W)\n        original_image = original_image.permute(0, 1, 2)  # Convert to (C, H, W)\n        # Return original, blurred images, and the label\n        return original_image, blurred_image, label\n\n# Data transformations for the Stanford Dogs Dataset\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),  # Resize images\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])\n\n# Load the Stanford Dogs Dataset\ntrain_data = datasets.ImageFolder(root='/kaggle/input/stanford-dogs-dataset-traintest/cropped/train/', transform=transform)\ntest_data = datasets.ImageFolder(root='/kaggle/input/stanford-dogs-dataset-traintest/cropped/test/', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:38:25.285555Z","iopub.execute_input":"2024-11-27T12:38:25.285834Z","iopub.status.idle":"2024-11-27T12:39:26.139750Z","shell.execute_reply.started":"2024-11-27T12:38:25.285808Z","shell.execute_reply":"2024-11-27T12:39:26.138708Z"},"trusted":true},"outputs":[],"execution_count":106},{"cell_type":"code","source":"# Define the number of samples you want for training and testing\nnum_train_samples = 30\nnum_test_samples = 500\n\n# Generate random indices for the training subset\ntrain_indices = torch.randperm(len(train_data)).tolist()[:num_train_samples]\ntrain_subset = torch.utils.data.Subset(train_data, train_indices)\n\n# Generate random indices for the testing subset\ntest_indices = torch.randperm(len(test_data)).tolist()[:num_test_samples]\ntest_subset = torch.utils.data.Subset(test_data, test_indices)\n\n# Create DataLoaders for the normal train/test sets\ntrain_loader = DataLoader(train_subset, batch_size=1, shuffle=True)\ntest_loader = DataLoader(test_subset, batch_size=1, shuffle=False)\n\n# Now create the blurred versions of these subsets using BlurredDogDataset\nblurred_train_dataset = BlurredDogDataset(train_subset)\nblurred_test_dataset = BlurredDogDataset(test_subset)\n\n# Create DataLoaders for the blurred datasets\nblurred_train_loader = DataLoader(blurred_train_dataset, batch_size=1, shuffle=True)\nblurred_test_loader = DataLoader(blurred_test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:39:26.140972Z","iopub.execute_input":"2024-11-27T12:39:26.141282Z","iopub.status.idle":"2024-11-27T12:39:26.150735Z","shell.execute_reply.started":"2024-11-27T12:39:26.141255Z","shell.execute_reply":"2024-11-27T12:39:26.150009Z"},"trusted":true},"outputs":[],"execution_count":107},{"cell_type":"code","source":"model = models.resnet50(pretrained=False) \nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(train_data.classes))  # Adjust for the number of classes\nmodel.load_state_dict(torch.load('/kaggle/input/stanford_dogs_resnet50/pytorch/default/1/resnet50_dog_model.pth'))\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:39:26.152394Z","iopub.execute_input":"2024-11-27T12:39:26.152672Z","iopub.status.idle":"2024-11-27T12:39:26.717481Z","shell.execute_reply.started":"2024-11-27T12:39:26.152647Z","shell.execute_reply":"2024-11-27T12:39:26.716586Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_105/910992481.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/stanford_dogs_resnet50/pytorch/default/1/resnet50_dog_model.pth'))\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"# Evaluate on test set and calculate accuracy score\nmodel.eval()  # Set model to evaluation mode\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# Calculate and print accuracy using sklearn's accuracy_score\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:39:26.718717Z","iopub.execute_input":"2024-11-27T12:39:26.719404Z","iopub.status.idle":"2024-11-27T12:39:34.130073Z","shell.execute_reply.started":"2024-11-27T12:39:26.719362Z","shell.execute_reply":"2024-11-27T12:39:34.129164Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.4680\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"# Evaluate on test set and calculate accuracy score\nmodel.eval()  # Set model to evaluation mode\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for _,inputs, labels in blurred_test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# Calculate and print accuracy using sklearn's accuracy_score\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:39:34.131357Z","iopub.execute_input":"2024-11-27T12:39:34.131736Z","iopub.status.idle":"2024-11-27T12:39:39.277900Z","shell.execute_reply.started":"2024-11-27T12:39:34.131695Z","shell.execute_reply":"2024-11-27T12:39:39.277017Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.1520\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Denormalize function\n# def denormalize(tensor, mean, std):\n#     # Denormalize each channel (C, H, W)\n#     for c in range(tensor.shape[1]):  # Loop over channels (C)\n#         tensor[:, c] = tensor[:, c] * std[c] + mean[c]\n#     return tensor\n\n# # Function to plot 5 images in a batch\n# def plot_images(batch, num_images=5):\n#     # Unpack the batch\n#     original_images, blurred_images, labels = batch\n    \n#     # Denormalize the images\n#     mean = torch.tensor([0.485, 0.456, 0.406])\n#     std = torch.tensor([0.229, 0.224, 0.225])\n    \n#     # Denormalize the blurred images (batch of images)\n#     blurred_images = denormalize(blurred_images.clone(), mean, std)\n    \n#     # Create a figure to hold the images\n#     fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n    \n#     # Plot each image in the batch\n#     for i in range(num_images):\n#         ax = axes[i]\n#         ax.imshow(blurred_images[i].permute(1, 2, 0).numpy())  # Convert (C, H, W) to (H, W, C)\n#         ax.axis('off')\n#         ax.set_title(f\"Label: {labels[i].item()}\")\n    \n#     plt.show()\n\n# # Fetch a batch of images from the blurred_train_loader and plot them\n# for batch in blurred_train_loader:\n#     plot_images(batch, num_images=2)\n#     break  # Just plot one batch","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:39:39.278902Z","iopub.execute_input":"2024-11-27T12:39:39.279204Z","iopub.status.idle":"2024-11-27T12:39:39.283630Z","shell.execute_reply.started":"2024-11-27T12:39:39.279177Z","shell.execute_reply":"2024-11-27T12:39:39.282609Z"},"trusted":true},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":"# **HOAG**","metadata":{}},{"cell_type":"code","source":"def conjugate_gradient(H, b, tol=1e-6, max_iter=100): #used to solve Hx = b, where H is function \n                                                      #that calculates Hx.\n    x = torch.zeros_like(b)\n    r = b.clone()\n    p = r.clone()\n    rsold = torch.sum(r*r)\n\n    for i in range(max_iter):\n        Hp = H(p)\n        alpha = rsold / torch.sum(p* Hp)\n        x = x + alpha * p\n        r = r - alpha * Hp\n        rsnew = torch.sum(r*r)\n\n        if torch.sqrt(rsnew) < tol:\n            break\n\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n\n    return x\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\n\n# Smoothed Total Variation Function\ndef Smoothed_TV(x, theta):\n    batch_size = x.size(0)\n    diff_x = x[:, :, :, 1:] - x[:, :, :, :-1]\n    diff_x = F.pad(diff_x, (0, 1), mode='constant', value=0)\n    diff_y = x[:, :, 1:, :] - x[:, :, :-1, :]\n    diff_y = F.pad(diff_y, (0, 0, 0, 1), mode='constant', value=0)\n    norm_grad = torch.sqrt(diff_x.pow(2) + diff_y.pow(2) + torch.exp(theta[1])**2)\n    regularizer = torch.exp(theta[0]) * norm_grad.view(batch_size, -1).mean()\n    return regularizer","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:26.961218Z","iopub.execute_input":"2024-11-27T12:54:26.961889Z","iopub.status.idle":"2024-11-27T12:54:26.969706Z","shell.execute_reply.started":"2024-11-27T12:54:26.961854Z","shell.execute_reply":"2024-11-27T12:54:26.968783Z"},"trusted":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":"class RegularizerScaler(torch.nn.Module):\n    def __init__(self, regu, initial_alpha=10**-5, kernel_size=7, num_kernels=10):\n        super(RegularizerScaler, self).__init__()\n        self.alpha = torch.nn.Parameter(torch.tensor(initial_alpha))\n        self.kernel_size = kernel_size\n        self.num_kernels = num_kernels\n        self.regu = regu\n\n    def forward(self, noisy_x, lambda0):\n        if self.regu == \"smoothed_tv\":\n            regularizer = Smoothed_TV(noisy_x, lambda0)\n        elif self.regu == \"FoE\":\n            regularizer = (10**-2)*foe_regularizer(noisy_x, lambda0, kernel_size=self.kernel_size, num_kernels=self.num_kernels)\n        return regularizer","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:27.532809Z","iopub.execute_input":"2024-11-27T12:54:27.533093Z","iopub.status.idle":"2024-11-27T12:54:27.538616Z","shell.execute_reply.started":"2024-11-27T12:54:27.533066Z","shell.execute_reply":"2024-11-27T12:54:27.537914Z"},"trusted":true},"outputs":[],"execution_count":120},{"cell_type":"code","source":"def inner_loss(noisy_x, x_star, lambda0, regularizer):\n    residual = noisy_x - x_star\n    loss = torch.mean(residual**2)\n    regularizer_term = regularizer.forward(noisy_x, lambda0)\n    return loss + regularizer_term\n\ndef inner_optimization(noisy_x, lambda0, x_star,regularizer,tol=1e-1, max_iter=100):\n    recons_x = noisy_x.requires_grad_(True)\n    \n    # Include the parameters of the regularizer (i.e., alpha) in the optimizer\n    optimizer = optim.LBFGS([recons_x] + list(regularizer.parameters()), lr=1e-1, max_iter=max_iter, tolerance_grad=tol, tolerance_change=tol)\n\n    def closure():\n        optimizer.zero_grad()\n        loss = inner_loss(noisy_x, x_star,lambda0,regularizer=regularizer).to(noisy_x.device)\n        loss.backward(retain_graph= True)\n        return loss\n\n    for _ in range(max_iter):\n        optimizer.step(closure)\n\n    return recons_x","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:28.038757Z","iopub.execute_input":"2024-11-27T12:54:28.039093Z","iopub.status.idle":"2024-11-27T12:54:28.045661Z","shell.execute_reply.started":"2024-11-27T12:54:28.039062Z","shell.execute_reply":"2024-11-27T12:54:28.044646Z"},"trusted":true},"outputs":[],"execution_count":121},{"cell_type":"code","source":"def compute_Hv(loss, p, u, v, flag=\"both\"): \n    \"\"\"\n    Compute the Hessian-vector product and the gradient.\n    \n    Parameters:\n    - loss: Precomputed scalar loss.\n    - p: The vector to multiply with the Hessian.\n    - u: Tensor with respect to which the first derivative is taken.\n    - v: Tensor with respect to which the Hessian is calculated.\n    - flag: Specifies what to return (\"grad\", \"both\", \"hess\").\n    \n    Returns:\n    - Depending on the flag, returns:\n      - \"grad\": Gradient of the loss with respect to u.\n      - \"both\": Gradient of the loss with respect to u and the Hessian-vector product.\n      - \"hess\": Hessian-vector product only.\n    \"\"\"\n    grad_u = autograd.grad(loss, u, create_graph=True)[0]\n    \n    if flag == \"grad\":\n        return grad_u\n    \n    elif flag == \"both\":\n        dell_u_times_p = torch.sum(grad_u * p)\n        Hv = autograd.grad(dell_u_times_p, v, retain_graph=True)[0]\n        return grad_u, Hv\n    \n    elif flag == \"hess\":\n        dell_u_times_p = torch.sum(grad_u * p)\n        Hv = autograd.grad(dell_u_times_p, v, retain_graph=True)[0]\n        return Hv\n\ndef create_H_function(loss, u, v):\n    \"\"\"\n    Creates a function H(p) that computes the Hessian-vector product for a given vector p.\n    \n    Parameters:\n    - loss: Precomputed scalar loss.\n    - u: Tensor with respect to which the first derivative is taken.\n    - v: Tensor with respect to which the Hessian is calculated.\n    \n    Returns:\n    - A function H(p) that computes the Hessian-vector product Hv.\n    \"\"\"\n    def H(p):\n        return compute_Hv(loss, p, u, v, flag=\"hess\")\n    \n    return H","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:29.146789Z","iopub.execute_input":"2024-11-27T12:54:29.147536Z","iopub.status.idle":"2024-11-27T12:54:29.154230Z","shell.execute_reply.started":"2024-11-27T12:54:29.147496Z","shell.execute_reply":"2024-11-27T12:54:29.153278Z"},"trusted":true},"outputs":[],"execution_count":122},{"cell_type":"code","source":"def hoag_algorithm(noisy_x, lambda0, x_star, labels, regu, \n                   T_star, device, max_iter=20, k=1.0, \n                   epsilon=1e-8,kernel_size=5,num_kernels=2):\n    \n    lambda0 = lambda0.to(device)\n    tol = 0.0001  # Initial tolerance\n\n    # Initialize RegularizerScaler\n    regularizer = RegularizerScaler(regu = regu, kernel_size=kernel_size, num_kernels=num_kernels).to(device)\n    # Step (i): Solve the inner optimization problem to get the initial gradient\n    recons_x = inner_optimization(noisy_x, lambda0, x_star,regularizer=regularizer, \n                                  tol=tol).to(device)\n    output = T_star(recons_x)\n    criterion = torch.nn.CrossEntropyLoss()\n    outer_loss = criterion(output, labels).to(device)\n    grad_g_x = autograd.grad(outer_loss, recons_x, create_graph=True)[0].to(device)\n    \n    # Compute the initial p1 (approximate gradient) and initialize L\n    p1 = torch.linalg.matrix_norm(grad_g_x).mean().item()\n    L = k * p1\n#     print(L)\n\n    for iteration in range(max_iter):\n        # Apply exponential decay to tol\n        tol = max(tol * (1 - iteration / max_iter)**5, epsilon)  # Ensure tol does not go below epsilon\n\n        # Step (i): Solve the inner optimization problem\n        recons_x = inner_optimization(noisy_x, lambda0, x_star, \n                                      regularizer=regularizer, tol=tol).to(device)\n        \n        # Compute the outer loss based on recons_x\n        output = T_star(recons_x)\n        outer_loss = criterion(output, labels).to(device)\n        \n        # Compute the gradient with respect to recons_x\n        grad_g_x = autograd.grad(outer_loss, recons_x, create_graph=True)[0].to(device)        \n        \n        # Step (ii): Compute the Hessian-vector product\n        new_inner_loss = inner_loss(noisy_x, x_star, lambda0, regularizer).to(device)\n        Hxx = create_H_function(new_inner_loss, recons_x, recons_x)\n        q_k = conjugate_gradient(Hxx, grad_g_x, tol=tol, max_iter=max_iter)\n        hxlambda = compute_Hv(new_inner_loss, q_k, recons_x, lambda0, flag=\"hess\").to(device)\n        p_k = -hxlambda\n        noisy_x = recons_x\n\n    return p_k, L, outer_loss,regularizer","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:29.877246Z","iopub.execute_input":"2024-11-27T12:54:29.877603Z","iopub.status.idle":"2024-11-27T12:54:29.886168Z","shell.execute_reply.started":"2024-11-27T12:54:29.877573Z","shell.execute_reply":"2024-11-27T12:54:29.885419Z"},"trusted":true},"outputs":[],"execution_count":123},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:31.473988Z","iopub.execute_input":"2024-11-27T12:54:31.474344Z","iopub.status.idle":"2024-11-27T12:54:31.478647Z","shell.execute_reply.started":"2024-11-27T12:54:31.474316Z","shell.execute_reply":"2024-11-27T12:54:31.477737Z"},"trusted":true},"outputs":[],"execution_count":124},{"cell_type":"markdown","source":"# **Smoothed_tv**","metadata":{}},{"cell_type":"code","source":"import time\nimport copy\nfrom tqdm import tqdm  # Import tqdm for progress bars\nimport torch\nimport torch.optim as optim\n\n# Initialize lambda0\n# Initialize lambda0 as a leaf tensor with values of 4\nlambda0 = 3*torch.ones(2, requires_grad=True, device=device)  # Initialize tensor with value 3\nregu = \"smoothed_tv\"\nsaved_images = []\n\n# Loss function\ncriterion = torch.nn.CrossEntropyLoss()\nT_star = model\n\nfor epoch in range(1):  # Adjust epochs as needed\n    # Training Phase\n    train_start_time = time.time()\n\n    # Initialize the accumulated gradient\n    accumulated_grad = torch.zeros_like(lambda0).to(device)\n\n    # Add tqdm for the training loop\n    train_loader = tqdm(blurred_train_loader, desc=f\"Training Epoch {epoch + 1}\")\n    total_train_loss = 0.0\n    total_samples = 0\n\n    for batch_idx, (gt, noisy, labels) in enumerate(train_loader):\n        gt, noisy, labels = gt.to(device), noisy.to(device), labels.to(device)\n\n        # Apply HOAG algorithm (inner optimization)\n        pk, L, loss, regularizer = hoag_algorithm(\n            noisy_x=noisy, lambda0=lambda0, x_star=gt,\n            labels=labels, regu=regu,\n            T_star=T_star, device=device, max_iter=3, k=1.0,\n            epsilon=1e-4, kernel_size=5, num_kernels=2)\n\n        # Update lambda0\n        L = 1 / L\n        lambda0 = lambda0 - L * pk\n\n        # Clamp lambda0 to the desired range after the update\n        lambda0.data = lambda0.data.clamp(min=-10, max=4)\n\n        # Track total loss and samples for average loss computation\n        total_train_loss += loss.item() * labels.size(0)  # Multiply by batch size\n        total_samples += labels.size(0)\n\n        # Calculate running average loss\n        avg_loss = total_train_loss / total_samples\n\n        # Update tqdm description with avg loss and current lambda0\n        train_loader.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\", \"lambda0\": lambda0.data.cpu().numpy()})\n\n    train_end_time = time.time()\n    train_elapsed_time = train_end_time - train_start_time\n\n# Evaluation Phase\nouter_loss = 0.0\ncorrect_predictions = 0\ntotal_samples = 0\nT_star.eval()\n\neval_start_time = time.time()\n\n# Add tqdm for the evaluation loop\ntest_loader = tqdm(blurred_test_loader, desc=f\"Evaluating Epoch {epoch + 1}\")\nwith torch.no_grad():\n    for (gt, noisy, labels) in test_loader:\n        gt, noisy, labels = gt.to(device), noisy.to(device), labels.to(device)\n        noisy_copy = copy.deepcopy(noisy)\n\n        # Solve inner optimization problem\n        recons_x = inner_optimization(noisy_x=noisy, lambda0=lambda0, x_star=gt,\n                                      tol=1e-4, regularizer=regularizer)\n        saved_images.append((recons_x.cpu(), gt.cpu(), noisy_copy.cpu()))\n\n        # Predict using T_star\n        output = T_star(recons_x)\n\n        # Compute the loss\n        loss = criterion(output, labels)\n        outer_loss += loss.item() * labels.size(0)  # Multiply by batch size for correct averaging\n\n        # Compute accuracy\n        _, predicted = torch.max(output, 1)\n        correct_predictions += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n        # Calculate running accuracy\n        running_accuracy = correct_predictions / total_samples * 100  # Percentage\n\n        # Update tqdm description with running accuracy\n        test_loader.set_postfix({\"Running Accuracy (%)\": f\"{running_accuracy:.2f}\"})\n\neval_end_time = time.time()\neval_elapsed_time = eval_end_time - eval_start_time\n\n# Print final results for the epoch\naverage_loss = outer_loss / len(blurred_test_loader.dataset)  # Normalize by number of samples\naccuracy = correct_predictions / total_samples * 100  # Final accuracy as a percentage\n\nprint(f\"Epoch {epoch + 1}\")\nprint(f\"Training Time: {train_elapsed_time:.2f}s\")\nprint(f\"Evaluation Time: {eval_elapsed_time:.2f}s\")\nprint(f\"Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:54:32.779150Z","iopub.execute_input":"2024-11-27T12:54:32.779493Z","iopub.status.idle":"2024-11-27T12:57:07.933602Z","shell.execute_reply.started":"2024-11-27T12:54:32.779462Z","shell.execute_reply":"2024-11-27T12:57:07.932663Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 30/30 [00:35<00:00,  1.18s/it, Avg Loss=2.8500, lambda0=[0.7228782 3.716454 ]]  \nEvaluating Epoch 1: 100%|██████████| 500/500 [01:59<00:00,  4.18it/s, Running Accuracy (%)=13.60]","output_type":"stream"},{"name":"stdout","text":"Epoch 1\nTraining Time: 35.46s\nEvaluation Time: 119.68s\nAverage Loss: 4.1442, Accuracy: 13.60%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":125},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Denormalize function\ndef denormalize(tensor, mean, std):\n    # Check the number of channels and denormalize accordingly\n    if tensor.shape[1] == 3:  # RGB image\n        for c in range(tensor.shape[1]):  # Loop over channels (C)\n            tensor[:, c] = tensor[:, c] * std[c] + mean[c]\n    elif tensor.shape[1] == 1:  # Grayscale image (1 channel)\n        tensor[:, 0] = tensor[:, 0] * std[0] + mean[0]\n    return tensor\n\n# Select the first set of images (recons_x, gt, noisy) for visualization\nrecons_x, gt, noisy = saved_images[0]\n\n# If the batch size is > 1, we need to select a single image from the batch\n# Assuming we want to visualize the first image in the batch\nrecons_x = recons_x[0]  # Select the first image\ngt = gt[0]              # Select the first image\nnoisy = noisy[0]        # Select the first image\n\n# Denormalize the images\nmean = torch.tensor([0.485, 0.456, 0.406])\nstd = torch.tensor([0.229, 0.224, 0.225])\n\nrecons_x = denormalize(recons_x.clone(), mean, std)\ngt = denormalize(gt.clone(), mean, std)\nnoisy = denormalize(noisy.clone(), mean, std)\n\n# Convert tensors to numpy for plotting\nrecons_x_np = recons_x.squeeze().detach().cpu().permute(1, 2, 0).numpy()\ngt_np = gt.squeeze().detach().cpu().permute(1, 2, 0).numpy()\nnoisy_np = noisy.squeeze().detach().cpu().permute(1, 2, 0).numpy()\n\n# Plotting the images\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(gt_np)\naxes[0].set_title('Ground Truth (GT)')\naxes[0].axis('off')\n\naxes[1].imshow(noisy_np)\naxes[1].set_title('Noisy Image')\naxes[1].axis('off')\n\naxes[2].imshow(recons_x_np)\naxes[2].set_title('Reconstructed Image')\naxes[2].axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-27T12:57:07.935476Z","iopub.execute_input":"2024-11-27T12:57:07.936253Z","iopub.status.idle":"2024-11-27T12:57:08.146498Z","shell.execute_reply.started":"2024-11-27T12:57:07.936211Z","shell.execute_reply":"2024-11-27T12:57:08.145653Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 3 Axes>","image/png":"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"},"metadata":{}}],"execution_count":126},{"cell_type":"markdown","source":"# **finetuning smoothed tv**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import autograd\n\ndef hoag_algorithm_with_finetuning(noisy_x, lambda0, x_star, labels, regu, \n                                   T_star, device, optimizer, max_iter=20, k=1.0, \n                                   epsilon=1e-6, kernel_size=5, num_kernels=2):\n    \n    # Define CrossEntropyLoss as the outer loss function\n    outer_loss_fn = nn.CrossEntropyLoss().to(device)\n\n    lambda0 = lambda0.to(device)\n    tol = 0.0001  # Initial tolerance\n\n    # Initialize RegularizerScaler\n    regularizer = RegularizerScaler(regu=regu, kernel_size=kernel_size, num_kernels=num_kernels).to(device)\n    \n    # Step (i): Solve the inner optimization problem to get the initial gradient\n    recons_x = inner_optimization(noisy_x, lambda0, x_star, regularizer=regularizer, \n                                   tol=tol).to(device)\n\n    for iteration in range(max_iter):\n        tol = max(tol * (1 - iteration / max_iter)**5, epsilon)  # Ensure tol does not go below epsilon\n\n        # Step (i): Solve the inner optimization problem\n        recons_x = inner_optimization(noisy_x, lambda0, x_star, \n                                       regularizer=regularizer, tol=tol).to(device)\n        \n        # Evaluate T_star to compute the outer loss\n        T_star.eval()\n        output = T_star(recons_x)\n        outer_loss = outer_loss_fn(output, labels).to(device)  \n        \n        # Compute the gradient with respect to recons_x\n        grad_g_x = autograd.grad(outer_loss, recons_x, create_graph=True)[0].to(device)        \n        \n        # Step (ii): Compute the Hessian-vector product\n        new_inner_loss = inner_loss(noisy_x, x_star, lambda0, regularizer).to(device)\n        Hxx = create_H_function(new_inner_loss, recons_x, recons_x)\n        q_k = conjugate_gradient(Hxx, grad_g_x, tol=tol, max_iter=max_iter)\n        hxlambda = compute_Hv(new_inner_loss, q_k, recons_x, lambda0, flag=\"hess\").to(device)\n        p_k = -hxlambda\n        noisy_x = recons_x\n    \n    # Final evaluation\n    T_star.eval()\n    output = T_star(recons_x)\n    outer_loss = outer_loss_fn(output, labels).to(device)    \n    grad_g_x = autograd.grad(outer_loss, recons_x, create_graph=True)[0].to(device)\n    p1 = torch.linalg.matrix_norm(grad_g_x).mean().item()\n    L = k * p1\n    \n    return p_k, L, outer_loss, regularizer, tol","metadata":{"execution":{"iopub.status.busy":"2024-11-27T13:13:50.778942Z","iopub.execute_input":"2024-11-27T13:13:50.779313Z","iopub.status.idle":"2024-11-27T13:13:50.789157Z","shell.execute_reply.started":"2024-11-27T13:13:50.779280Z","shell.execute_reply":"2024-11-27T13:13:50.788127Z"},"trusted":true},"outputs":[],"execution_count":139},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torchvision.models import resnet50\n\n# Load your ResNet-50 model\nT_star_smtv = models.resnet50(pretrained=False) \nnum_ftrs = T_star_smtv.fc.in_features\nT_star_smtv.fc = nn.Linear(num_ftrs, len(train_data.classes))  # Adjust for the number of classes\nT_star_smtv.load_state_dict(torch.load('/kaggle/input/stanford_dogs_resnet50/pytorch/default/1/resnet50_dog_model.pth'))\nT_star_smtv = T_star_smtv.to(device)\n\n# Freeze all parameters first\nfor param in T_star_smtv.parameters():\n    param.requires_grad = False\n\n# Unfreeze parameters in the fully connected (fc) layer\nfor param in T_star_smtv.fc.parameters():\n    param.requires_grad = True\n\n# Unfreeze parameters in layer4 (conv5 in your terminology)\nfor param in T_star_smtv.layer4.parameters():\n    param.requires_grad = True\n\n# Initialize the Adam optimizer for the trainable parameters\noptimizer = optim.Adam(\n    list(T_star_smtv.fc.parameters())+ list(T_star_smtv.layer4.parameters()),\n    lr=0.00001\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-27T13:13:52.283168Z","iopub.execute_input":"2024-11-27T13:13:52.283536Z","iopub.status.idle":"2024-11-27T13:13:52.830229Z","shell.execute_reply.started":"2024-11-27T13:13:52.283502Z","shell.execute_reply":"2024-11-27T13:13:52.829248Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_105/722786877.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  T_star_smtv.load_state_dict(torch.load('/kaggle/input/stanford_dogs_resnet50/pytorch/default/1/resnet50_dog_model.pth'))\n","output_type":"stream"}],"execution_count":140},{"cell_type":"code","source":"import time\nimport copy\nimport torch\nfrom tqdm import tqdm  # Import tqdm\n\n# Initialize lambda0\nlambda0 = 3*torch.ones(2, requires_grad=True, device=device)\nregu = \"smoothed_tv\"\nsaved_images = []\n# Loss function\ncriterion = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(5):  # Adjust epochs as needed\n    # Training Phase\n    train_start_time = time.time()\n    \n    # Initialize variables for loss tracking\n    total_outer_loss = 0.0\n    batch_count = 0\n\n    # Add tqdm for the training phase\n    train_loader = tqdm(blurred_train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n    for (gt, noisy, labels) in train_loader:\n        gt, noisy, labels = gt.to(device), noisy.to(device), labels.long().to(device)\n        \n        pk, L, loss, regularizer,tol = hoag_algorithm_with_finetuning(\n            noisy_x=noisy, lambda0=lambda0, x_star=gt, optimizer=optimizer,\n            labels=labels, regu=regu, \n            T_star=T_star_smtv, device=device, max_iter=3, k=1.0,\n            epsilon=1e-4, kernel_size=5, num_kernels=2)\n\n        # Update total outer loss\n        total_outer_loss += loss.item()\n        batch_count += 1\n\n        # Update lambda0\n#         if L * torch.linalg.vector_norm(pk) < 1:\n#         print(L)\n        L=1/L\n        lambda0 = lambda0 - 0.01 * pk\n        lambda0 = lambda0.clamp(min=-10, max=4)\n        \n        # Perform model training\n        T_star_smtv.train()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Print iteration details\n        train_loader.set_postfix(lambda0=lambda0.data.cpu().numpy(), loss=loss.item(),tolerance=tol)\n\n    train_end_time = time.time()\n    train_elapsed_time = train_end_time - train_start_time\n\n    # Calculate average loss for the epoch\n    average_loss = total_outer_loss / batch_count if batch_count > 0 else 0.0\n    print(f\"Average loss: {average_loss}, Epoch: {epoch}\")    ","metadata":{"execution":{"iopub.status.busy":"2024-11-27T13:13:57.309923Z","iopub.execute_input":"2024-11-27T13:13:57.310556Z","iopub.status.idle":"2024-11-27T13:20:58.097074Z","shell.execute_reply.started":"2024-11-27T13:13:57.310519Z","shell.execute_reply":"2024-11-27T13:20:58.096179Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Average loss: 2.3209204058628528, Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Average loss: 3.7993839543312786, Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Average loss: 3.0378214955329894, Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Average loss: 1.6177144854019085, Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Average loss: 0.5609872522453467, Epoch: 4\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\n\n# Evaluation Phase\nouter_loss = 0.0\ncorrect_predictions = 0\ntotal_samples = 0\nT_star_smtv.eval()\n\neval_start_time = time.time()\nsaved_images = []  # Initialize saved images list\n\n# Wrap the test loader with tqdm for a progress bar\nwith torch.no_grad():\n    progress_bar = tqdm(blurred_test_loader, desc=\"Evaluating\", unit=\"batch\")\n    for (gt, noisy, labels) in progress_bar:\n        gt, noisy, labels = gt.to(device), noisy.to(device), labels.to(device)\n        noisy_copy = copy.deepcopy(noisy)\n\n        # Solve inner optimization problem\n        recons_x = inner_optimization(noisy_x=noisy, lambda0=lambda0, x_star=gt, \n                                      tol=1e-4, regularizer=regularizer)\n        saved_images.append((recons_x.cpu(), gt.cpu(), noisy_copy.cpu()))\n\n        # Predict using T_star\n        output = T_star_smtv(recons_x)\n\n        # Compute the loss\n        loss = criterion(output, labels)\n        outer_loss += loss.item() * labels.size(0)  # Multiply by batch size for correct averaging\n\n        # Compute accuracy\n        _, predicted = torch.max(output, 1)\n        correct_predictions += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n        # Update running accuracy and loss in progress bar\n        running_accuracy = correct_predictions / total_samples * 100  # Running accuracy as percentage\n        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{running_accuracy:.2f}%\")\n\neval_end_time = time.time()\neval_elapsed_time = eval_end_time - eval_start_time\n                \n# Calculate final metrics\naverage_loss = outer_loss / len(blurred_test_loader.dataset)  # Normalize by number of samples\naccuracy = correct_predictions / total_samples * 100  # Accuracy as a percentage\n\n# Print results\nprint(f\"Epoch {epoch + 1}\")\nprint(f\"Training Time: {train_elapsed_time:.2f}s\")\nprint(f\"Evaluation Time: {eval_elapsed_time:.2f}s\")\nprint(f\"Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-11-27T13:25:05.178279Z","iopub.execute_input":"2024-11-27T13:25:05.178991Z","iopub.status.idle":"2024-11-27T13:25:20.564702Z","shell.execute_reply.started":"2024-11-27T13:25:05.178958Z","shell.execute_reply":"2024-11-27T13:25:20.563495Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Evaluating:   2%|▏         | 11/500 [00:15<11:19,  1.39s/batch, acc=0.00%, loss=4.2787]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[144], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m noisy_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(noisy)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Solve inner optimization problem\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m recons_x \u001b[38;5;241m=\u001b[39m \u001b[43minner_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_star\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m saved_images\u001b[38;5;241m.\u001b[39mappend((recons_x\u001b[38;5;241m.\u001b[39mcpu(), gt\u001b[38;5;241m.\u001b[39mcpu(), noisy_copy\u001b[38;5;241m.\u001b[39mcpu()))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Predict using T_star\u001b[39;00m\n","Cell \u001b[0;32mIn[121], line 20\u001b[0m, in \u001b[0;36minner_optimization\u001b[0;34m(noisy_x, lambda0, x_star, regularizer, tol, max_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m---> 20\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recons_x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lbfgs.py:323\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    320\u001b[0m state\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    325\u001b[0m current_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[121], line 16\u001b[0m, in \u001b[0;36minner_optimization.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m inner_loss(noisy_x, x_star,lambda0,regularizer\u001b[38;5;241m=\u001b[39mregularizer)\u001b[38;5;241m.\u001b[39mto(noisy_x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":144},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Denormalize function\ndef denormalize(tensor, mean, std):\n    # Check the number of channels and denormalize accordingly\n    if tensor.shape[1] == 3:  # RGB image\n        for c in range(tensor.shape[1]):  # Loop over channels (C)\n            tensor[:, c] = tensor[:, c] * std[c] + mean[c]\n    elif tensor.shape[1] == 1:  # Grayscale image (1 channel)\n        tensor[:, 0] = tensor[:, 0] * std[0] + mean[0]\n    return tensor\n\n# Select the first set of images (recons_x, gt, noisy) for visualization\nrecons_x, gt, noisy = saved_images[0]\n\n# If the batch size is > 1, we need to select a single image from the batch\n# Assuming we want to visualize the first image in the batch\nrecons_x = recons_x[0]  # Select the first image\ngt = gt[0]              # Select the first image\nnoisy = noisy[0]        # Select the first image\n\n# Denormalize the images\nmean = torch.tensor([0.485, 0.456, 0.406])\nstd = torch.tensor([0.229, 0.224, 0.225])\n\nrecons_x = denormalize(recons_x.clone(), mean, std)\ngt = denormalize(gt.clone(), mean, std)\nnoisy = denormalize(noisy.clone(), mean, std)\n\n# Convert tensors to numpy for plotting\nrecons_x_np = recons_x.squeeze().detach().cpu().permute(1, 2, 0).numpy()\ngt_np = gt.squeeze().detach().cpu().permute(1, 2, 0).numpy()\nnoisy_np = noisy.squeeze().detach().cpu().permute(1, 2, 0).numpy()\n\n# Plotting the images\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(gt_np)\naxes[0].set_title('Ground Truth (GT)')\naxes[0].axis('off')\n\naxes[1].imshow(noisy_np)\naxes[1].set_title('Noisy Image')\naxes[1].axis('off')\n\naxes[2].imshow(recons_x_np)\naxes[2].set_title('Reconstructed Image')\naxes[2].axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-27T13:23:36.364809Z","iopub.status.idle":"2024-11-27T13:23:36.365086Z","shell.execute_reply.started":"2024-11-27T13:23:36.364951Z","shell.execute_reply":"2024-11-27T13:23:36.364965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}